{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a59bfb-6d7c-45dd-86cf-7be79577f1d4",
   "metadata": {},
   "source": [
    "# Case Study: Navigating Vanguard's Digital Redesign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbe8b9-4cc6-4be8-9088-18832c6e8314",
   "metadata": {},
   "source": [
    "Note: This is one of many possible solutions. This should serve as a guide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40962c5-ea5e-4d74-a5d8-06c8769bf6ae",
   "metadata": {},
   "source": [
    "As we saw in Part I, we need to deal with duplicates and null values.\n",
    "\n",
    "Ideally, we would have saved that code in functions, and here we would just call the functions, or we would have saved the clean datasets either as CSVs or as pickles.\n",
    "\n",
    "Since there is not much code regarding data cleaning as our main focus is not that one, we will just rewrite here the code below reading the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac26c48-62a7-4390-931c-ae7372a50b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = 'https://github.com/data-bootcamp-v4/lessons/raw/main/6_inf_stats/files_for_lessons/'\n",
    "\n",
    "# Correctly parsing the dataframes\n",
    "df_demo = pd.read_csv(path+'df_final_demo.txt', sep=\",\")\n",
    "df_experiment_clients = pd.read_csv(path+'df_final_experiment_clients.txt', sep=\",\")\n",
    "df_web_data_pt_1 = pd.read_csv(path+'df_final_web_data_pt_1.txt', sep=\",\")\n",
    "df_web_data_pt_2 = pd.read_csv(path+'df_final_web_data_pt_2.txt', sep=\",\")\n",
    "\n",
    "# Combining web data\n",
    "df_web_data = pd.concat([df_web_data_pt_1, df_web_data_pt_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22baa9a9-181e-4d2d-81ff-e2142c8ce5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are duplicates, we will drop them\n",
    "if duplicates[\"df_demo\"] > 0:\n",
    "    df_demo.drop_duplicates(inplace=True)\n",
    "if duplicates[\"df_experiment_clients\"] > 0:\n",
    "    df_experiment_clients.drop_duplicates(inplace=True)\n",
    "if duplicates[\"df_web_data\"] > 0:\n",
    "    df_web_data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Cleaning data\n",
    "    \n",
    "df_demo['gendr'].fillna(df_demo['gendr'].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9536acb8-f601-4fcd-8e33-d6ce15dc2a48",
   "metadata": {},
   "source": [
    "## **1. Client Behavior Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5248d120-5ee3-4406-9c30-2d61ef150167",
   "metadata": {},
   "source": [
    "### **Engagement Demographics**\n",
    "\n",
    "    - Who are the primary clients using this online process? Are they younger or older, new or long-standing clients? Do a client behaviour analysis to answer any relevant questions you might think of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5c8662-4197-41bc-b61b-fa090fcc3bd4",
   "metadata": {},
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195f839-95be-4b1a-a8ee-60134c7ea1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the age distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df_demo['clnt_age'], bins=20, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Client Age')\n",
    "plt.ylabel('Number of Clients')\n",
    "plt.title('Distribution of Client Ages')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eacaf5b-05b3-4543-8915-e5f4e80fccc0",
   "metadata": {},
   "source": [
    "The age distribution reveals that the majority of clients are between the ages of 20 and 70, with a peak around the 30s and the 45s to 65s approximately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13daf7-2a4c-49f0-bbb9-51a6a58d1f69",
   "metadata": {},
   "source": [
    "#### Tenure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434e7b2-fd13-469f-abfb-e6bd7ecdb1c6",
   "metadata": {},
   "source": [
    "Let's examine the distribution of client tenure to determine if they are new or long-standing clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb546f-5f64-4d24-ae95-59473758f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the client tenure distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df_demo['clnt_tenure_yr'], bins=20, edgecolor='k', alpha=0.7)\n",
    "plt.xlabel('Client Tenure (in years)')\n",
    "plt.ylabel('Number of Clients')\n",
    "plt.title('Distribution of Client Tenure')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2781d173-3d2f-4e5d-a51f-17e322cf42cd",
   "metadata": {},
   "source": [
    "The client tenure distribution indicates that a significant portion of clients have been with the platform for less than 10 years, with a peak around 5-7 years of tenure. This suggests that there are many relatively newer clients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745dbce-1f64-43c4-91d7-51f7dd806fa4",
   "metadata": {},
   "source": [
    "## 2. Performance Metrics\n",
    "\n",
    "- **Success Indicators**:\n",
    "    - What key performance indicators (KPIs) will determine the success of the new design? (e.g., completion rate, time spent on each step, error rates)\n",
    "- **Redesign Outcome**:\n",
    "    - Based on the chosen KPIs, how does the new design's performance compare to the old one?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877bce10-1376-4c7a-9c81-e53dd16846ec",
   "metadata": {},
   "source": [
    "To determine the success of the new design, we need to identify key performance indicators (KPIs). Given the data, potential KPIs include:\n",
    "\n",
    "- **Completion Rate**: The proportion of users who reach the final 'confirm' step.\n",
    "- **Time Spent on Each Step**: The average duration users spend on each step before moving to the next.\n",
    "- **Error Rates**: If there's a step where users often go back to a previous step, it may indicate confusion or an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be7534-593f-48b6-aad3-feec325c2189",
   "metadata": {},
   "source": [
    "**Redesign Outcome:**\n",
    "\n",
    "After selecting the KPIs, we'll calculate these metrics for both the Test and Control groups to compare the performance of the new design against the old one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b360c8b-a362-4fcd-b8c2-c634cdc294aa",
   "metadata": {},
   "source": [
    "### Completion Rate\n",
    "\n",
    "Let's start by calculating the **Completion Rate** for both the Test and Control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700531a-eaad-4d70-a2ce-bca0ec249592",
   "metadata": {},
   "source": [
    "    - For each group (Test and Control), let's calculate the number of users who reached the 'confirm' step and divide it by the total number of users in that group.\n",
    "    - This gives us the proportion (or probability) of users completing the process. \n",
    "    - Mathematically, for the Test group, the completion rate is given by:\n",
    "$$\n",
    "\\text{Completion Rate (Test)} = \\frac{\\text{Number of 'Test' users reaching 'confirm'}}{\\text{Total 'Test' users}}\n",
    "$$\n",
    "\n",
    "    - The same formula applies for the Control group. This is a direct application of probability where we determine the likelihood of an event (completion) occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ed30d-4514-4dcd-8419-f805f76241c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the web data with the experiment clients data to know which group each client belongs to\n",
    "df_web_experiment_merged = df_web_data.merge(df_experiment_clients, on='client_id', how='left')\n",
    "\n",
    "# Filter out the rows where the process step is 'confirm'\n",
    "df_confirmations = df_web_experiment_merged[df_web_experiment_merged['process_step'] == 'confirm']\n",
    "\n",
    "# Calculate completion rate for both Test and Control groups\n",
    "completion_rates = df_confirmations['Variation'].value_counts() / df_experiment_clients['Variation'].value_counts()\n",
    "\n",
    "completion_rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c463e23-89bc-47eb-9e7a-8d066483ed0a",
   "metadata": {},
   "source": [
    "The calculated completion rates for both the Test and Control groups are:\n",
    "\n",
    "- **Test Group (New Design)**: Approximately 94.92%\n",
    "- **Control Group (Old Design)**: Approximately 73.66%\n",
    "\n",
    "This indicates that users exposed to the new design were more likely to reach the final 'confirm' step compared to users exposed to the old design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85caf532-f002-4dbb-8f8b-cc19eef7c58e",
   "metadata": {},
   "source": [
    "### Time Spent on Each Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7593c50-ace1-4f26-bf23-3ea7a5898691",
   "metadata": {},
   "source": [
    "Next, let's calculate the **Time Spent on Each Step**. We'll determine the time difference between each step for each visit and then calculate the average duration users spend on each step before moving to the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d602149-4117-4861-8779-583bdb18e79c",
   "metadata": {},
   "source": [
    "The result will provide insights into the average time users of both the Test (new design) and Control (old design) groups spend on each of the process steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afdbded-3f15-4e1c-9207-a4056ef833fe",
   "metadata": {},
   "source": [
    "\n",
    "    - For each visit, we calculate the time difference between consecutive steps.\n",
    "    - We then average these time differences for each step across all visits.\n",
    "    - This does not directly use probability, but averages (or means) to understand typical user behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff316d2-d7a4-4824-8b5c-19ca8c81f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date_time' column to datetime format\n",
    "df_web_experiment_merged['date_time'] = pd.to_datetime(df_web_experiment_merged['date_time'])\n",
    "\n",
    "# Sort the data by client_id, visit_id, and date_time to ensure steps are in order\n",
    "df_web_experiment_merged = df_web_experiment_merged.sort_values(by=['client_id', 'visit_id', 'date_time'])\n",
    "\n",
    "# Calculate the time difference between each step for each visit\n",
    "df_web_experiment_merged['time_diff'] = df_web_experiment_merged.groupby(['client_id', 'visit_id'])['date_time'].diff()\n",
    "\n",
    "# Calculate the average duration users spend on each step for both Test and Control groups\n",
    "average_time_per_step = df_web_experiment_merged.groupby(['Variation', 'process_step'])['time_diff'].mean()\n",
    "\n",
    "# Convert the time difference to minutes for easier interpretation\n",
    "average_time_per_step = average_time_per_step.dt.total_seconds() / 60\n",
    "\n",
    "# Reset index for better presentation\n",
    "average_time_per_step = average_time_per_step.reset_index()\n",
    "\n",
    "average_time_per_step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67be19-da96-4848-80f9-cb0d43a14782",
   "metadata": {},
   "source": [
    "The results show the average time (in minutes) users spend on each step for both the Test and Control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e27c13-e007-49c3-adba-e12eba1451b4",
   "metadata": {},
   "source": [
    "### Error Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945bb7d4-c988-418f-b42f-ddbc81eb2b23",
   "metadata": {},
   "source": [
    "To address the **Error Rates KPI**, one approach is to identify instances where users go back to a previous step, suggesting possible confusion or an error.\n",
    "\n",
    "Let's calculate the error rates by identifying these instances. We'll consider moving from a later step to an earlier one as an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9db0a-8f5e-4132-8895-d8abb4d6f172",
   "metadata": {},
   "source": [
    "\n",
    "    - We identify instances where users moved from a later step to an earlier one (indicating possible confusion or errors).\n",
    "    - For each group, the error rate is calculated as the proportion of these \"error\" instances to the total number of steps taken.\n",
    "    - This is another application of probability, where we determine the likelihood of an error occurring. \n",
    "    - Mathematically, for the Test group, the error rate is given by:\n",
    "\n",
    "$$\n",
    "\\text{Error Rate (Test)} = \\frac{\\text{Number of 'backward' steps taken by 'Test' users}}{\\text{Total steps taken by 'Test' users}}\n",
    "$$\n",
    "\n",
    "    - The same formula applies for the Control group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b9d05-8133-4a69-81b9-f7bd38544640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We identify instances where users moved from a later step to an earlier one \n",
    "# (indicating possible confusion or errors).\n",
    "\n",
    "# Assign a numerical value to each process step for easier comparison\n",
    "step_mapping = {'start': 0, 'step_1': 1, 'step_2': 2, 'step_3': 3, 'confirm': 4}\n",
    "df_web_experiment_merged['step_value'] = df_web_experiment_merged['process_step'].map(step_mapping)\n",
    "\n",
    "# Calculate the step difference for each consecutive action within a visit\n",
    "df_web_experiment_merged['step_diff'] = df_web_experiment_merged.groupby(['client_id', 'visit_id'])['step_value'].diff()\n",
    "\n",
    "# Identify errors where the step difference is negative (i.e., moving from a later step to an earlier one)\n",
    "df_web_experiment_merged['is_error'] = df_web_experiment_merged['step_diff'] < 0\n",
    "\n",
    "# Calculate error rates for both Test and Control groups\n",
    "error_rates = df_web_experiment_merged.groupby('Variation')['is_error'].mean()\n",
    "\n",
    "error_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498856bc-36b5-405e-a983-7b709505875e",
   "metadata": {},
   "source": [
    "The calculated error rates for both the Test and Control groups are:\n",
    "\n",
    "- **Test Group (New Design)**: Approximately 9.1%\n",
    "- **Control Group (Old Design)**: Approximately 6.8%\n",
    "\n",
    "This suggests that users exposed to the new design experienced a slightly higher rate of errors (or instances where they went back to a previous step) compared to users exposed to the old design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be420801-2cda-4ae9-aa62-a10ba8ebb421",
   "metadata": {},
   "source": [
    "**Note**: The 'is_error' column is a binary indicator where `1` means an error (backward step) occurred, and `0` means no error. \n",
    "\n",
    "When you take the mean of a binary column:\n",
    "- The numerator is the sum of `1`s (which represents the number of 'backward' steps).\n",
    "- The denominator is the total number of rows (or the total number of steps taken by the group).\n",
    "\n",
    "This aligns with the provided formula above. The `.mean()` method on a binary column in pandas calculates the proportion of `1`s to the total, which in this case is the error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2548d9f6-edd2-41c0-a13c-2b627d6d9cf2",
   "metadata": {},
   "source": [
    "**Summary**:\n",
    "1. The **Completion Rate** for the new design is higher than the old design, indicating better user engagement or clarity with the new design.\n",
    "2. The **Time Spent on Each Step** it's a valuable metric for understanding user engagement and potential bottlenecks in the process.\n",
    "3. The **Error Rates** suggest that users of the new design had a slightly higher likelihood of going back to a previous step, possibly indicating confusion or hesitation.\n",
    "\n",
    "Based on these KPIs, the new design **seems** to improve completion rates but may introduce some points of confusion that cause users to revisit previous steps. Further analysis or user feedback might be needed to understand the reasons behind these behaviors and refine the design accordingly.\n",
    "Furthermore, we need to conduct hypothesis testing to make data-driven conclusions about the effectiveness of the redesign.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d97a41d-3db4-4d1c-a11e-e301d281f934",
   "metadata": {},
   "source": [
    "## **3. Hypothesis Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4986a372-ee86-4ddb-9986-d330d9bcaa15",
   "metadata": {},
   "source": [
    "As part of your analysis, you'll conduct hypothesis testing to make data-driven conclusions about the effectiveness of the redesign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b05a35-948c-4623-885e-891df43e8154",
   "metadata": {},
   "source": [
    "### Completion rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14773ffb-87d4-4d3f-a27e-d6279e337269",
   "metadata": {},
   "source": [
    "#### Before Conducting a Hypothesis Test\n",
    "\n",
    "- **Step 1: Define the Metric and Threshold.** \n",
    "    - In our case, the primary metric is the completion rate. There is no threshold.\n",
    "\n",
    "- **Step 2: Compute the Observed Completion Rates for Test and Control Groups**\n",
    "    - We'll first calculate the completion rates for both the Test and Control groups using unique clients. This was already above.\n",
    "\n",
    "- **Step 3: Determine the Observed Difference**\n",
    "    - After calculating the completion rates for both groups, we'll determine the observed difference between them to see if the new design (Test group) resulted in an improvement. This was done above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3141cb-86e2-4e91-a0d8-157df0323729",
   "metadata": {},
   "source": [
    "#### 1. **State the Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa364a15-478f-4b27-a1b7-7892b64ca5e9",
   "metadata": {},
   "source": [
    "In our case, the primary metric is the completion rate. Since the new design (Test group) had a higher completion rate compared to the old design (Control group), we might be interested in confirming if this difference is statistically significant.\n",
    "\n",
    "**Hypothesis**:\n",
    "- **Null Hypothesis ($H_0$))**: The completion rate for the Test group (new design) is equal to the completion rate for the Control group (old design).\n",
    "- **Alternative Hypothesis ($H_a$))**: The completion rate for the Test group (new design) is not equal to the completion rate for the Control group (old design).\n",
    "\n",
    "To test this hypothesis, we will use a two-proportion z-test. This test is appropriate when comparing proportions (like completion rates) between two groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daba3351-6a1a-4d40-9059-3030460f160b",
   "metadata": {},
   "source": [
    "#### 2. **Choose the Right Statistical Test**\n",
    "\n",
    "Given that we are comparing proportions between two groups, a two-proportion z-test is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb461deb-d393-42ad-bb21-1fe8c7c6a1be",
   "metadata": {},
   "source": [
    "#### 3. Test Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bddc29a-9927-485d-b669-40ce340846f4",
   "metadata": {},
   "source": [
    "The two-proportion *z-test* has certain assumptions that need to be met. Let's check these assumptions for our data:\n",
    "\n",
    "1. **Random Sampling**: The data should be randomly sampled, which means every member of the population has an equal chance of being included in the sample. \n",
    "    - This assumption often relies on the method of data collection. We don't have explicit information about the sampling method from the provided data, so we'll have to rely on external context. Typically, in experiments like these, users are randomly assigned to Test or Control groups, so this assumption might be reasonable.\n",
    "\n",
    "2. **Large Sample Size**: Both sample sizes should be large enough. We'll check this below.\n",
    "   \n",
    "3. **Independence**: The samples should be independent of each other. If sampling without replacement, the sample size should not be more than 10% of the population to ensure independence.\n",
    "    \n",
    "    - Again, this often depends on the data collection method. If users were randomly assigned to Test or Control groups, their experiences and outcomes would be independent of each other. However, without explicit information, this is an assumption we're making.\n",
    "\n",
    "Given the checks we've performed and typical practices in A/B testing, it seems reasonable to proceed with the two-proportion *z-test*, but always with the understanding that our conclusions are as good as the assumptions behind the test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7c141b-5c1a-4b98-b64b-c10d8eabf193",
   "metadata": {},
   "source": [
    "##### Large Sample Size check - rule of thumb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9fc124-4e83-482d-900d-300afd20bad9",
   "metadata": {},
   "source": [
    "The **Central Limit Theorem (CLT)** tells us that, for large sample sizes, the sampling distribution of the sample proportion is approximately normally distributed, regardless of the distribution of the underlying data.\n",
    "\n",
    "While we don't **check the normality** of our raw data in the same way we might for other tests (e.g., using a **Shapiro-Wilk** test or a **QQ plot**, since data is not continuous), we do ensure that our sample size and proportions are such that the **sampling distribution of the proportion is approximately normal**.\n",
    "\n",
    "In the case of the binomial distribution (which is the underlying distribution for proportions), we can check if it can be approximated by a normal distribution by  checking these conditions:\n",
    "$np>=5$ and $n(1-p)>=5$\n",
    "\n",
    "where \n",
    "     $n$ is total sample size and\n",
    "     $p$ is proportion of successes for both the Test and Control groups.\n",
    "\n",
    "\n",
    "These conditions are commonly taught rules of thumb for binomial distributions in introductory statistics courses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3710c3b-4d79-4e11-9108-b2be9b156c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of users and successes for each group\n",
    "grouped_data = df_web_experiment_merged.groupby('Variation').agg(\n",
    "    total_users=('client_id', 'nunique'),\n",
    "    successes=('process_step', lambda x: (x == 'confirm').sum())\n",
    ").reset_index()\n",
    "\n",
    "# Calculate the proportion of successes\n",
    "grouped_data['p'] = grouped_data['successes'] / grouped_data['total_users']\n",
    "\n",
    "# Calculate the conditions np>=5 and n(1-p)>=5\n",
    "grouped_data['np'] = grouped_data['total_users'] * grouped_data['p']\n",
    "grouped_data['n(1-p)'] = grouped_data['total_users'] * (1 - grouped_data['p'])\n",
    "\n",
    "grouped_data[['Variation', 'total_users', 'successes', 'p', 'np', 'n(1-p)']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0a569a-daab-420e-bb2e-df37a697531f",
   "metadata": {},
   "source": [
    "\n",
    "The conditions ```np>=5 and n(1-p)>=5``` for both the Test and Control groups have been calculated.\n",
    "\n",
    "Both groups meet the conditions, suggesting that the sample sizes are sufficiently large. This means we can reasonably proceed with the two-proportion z-test, under the assumption that the sampling distribution of the sample proportion is approximately normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5a60c-eb77-404a-9b03-32dbb064659c",
   "metadata": {},
   "source": [
    "#### 4. Conduct the test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b67993-7b39-419f-961e-7cc7376bca4f",
   "metadata": {},
   "source": [
    "Let's conduct the hypothesis test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b58887-62e3-43c0-924f-96c660321475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# Number of successes (completions) for each group\n",
    "count = df_confirmations['Variation'].value_counts()\n",
    "\n",
    "# Number of trials (total users) for each group\n",
    "nobs = df_experiment_clients['Variation'].value_counts()\n",
    "\n",
    "# Conduct two-proportion z-test\n",
    "z_stat, p_value = proportions_ztest(count, nobs)\n",
    "\n",
    "z_stat, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983bde22-97bc-4002-a530-42e745aa7bd2",
   "metadata": {},
   "source": [
    "The results from the two-proportion z-test are as follows:\n",
    "\n",
    "- **Z-statistic**: 66.77\n",
    "- **p-value**: 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12ad43b-f5f7-45d3-bfa2-0a692b64d853",
   "metadata": {},
   "source": [
    "#### 5. **Interpretation and decision**\n",
    "\n",
    "- Given the very low p-value (essentially 0), we can **reject the null hypothesis $H_0$** at any conventional significance level (e.g., $\\alpha = 0.05$). This means there is statistically significant evidence to suggest that the completion rate for the Test group (new design) is different from the Control group (old design).\n",
    "\n",
    "- Given our previous calculations, we know the **completion rate for the Test group is higher**. Therefore, our analysis supports the claim that the new design leads to a significantly higher completion rate compared to the old design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb7c55-36b5-437e-b6f6-5aa2c40ada96",
   "metadata": {},
   "source": [
    "- Given the statistically significant improvement in the completion rate, Vanguard **should implement the new UI design** as it has proven to be statistically better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4397c1-4364-48a7-9ad3-6f5a4e1e5990",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Completion rate with a Cost-Effectiveness Threshold\n",
    "\n",
    "The introduction of a new UI design comes with its associated costs: design, development, testing, potential training for staff, and possible short-term disruptions or adjustments for users. To justify these costs, Vanguard has determined that any new design should lead to a minimum increase in the completion rate to be deemed cost-effective.\n",
    "\n",
    "**Threshold**: Vanguard has set this minimum increase in completion rate at **5%**. This is the rate at which the projected benefits, in terms of increased user engagement and potential revenue, are estimated to outweigh the costs of the new design.\n",
    "\n",
    "Do another analysis, ensuring that the observed increase in completion rate from the A/B test meets or exceeds this **5%** threshold. If the new design doesn't lead to at least this level of improvement, it may not be justifiable from a cost perspective, regardless of its statistical significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61beea-950f-4526-9657-032d882e19f6",
   "metadata": {},
   "source": [
    "#### Before Conducting a Hypothesis Test\n",
    "\n",
    "- **Step 1: Define the Metric and Threshold.** \n",
    "    - In our case, the primary metric is the completion rate. The minimum increase in completion rate that Vanguard deems necessary to justify the costs associated with the new UI design is 5%.\n",
    "\n",
    "- **Step 2: Compute the Observed Completion Rates for Test and Control Groups**\n",
    "    - We'll first calculate the completion rates for both the Test and Control groups using unique clients. This was already done in the test above.\n",
    "\n",
    "- **Step 3: Determine the Observed Difference**\n",
    "    - After calculating the completion rates for both groups, we'll determine the observed difference between them to see if the new design (Test group) resulted in an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30073a10-8313-4a1b-b6de-968d6ec4553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the observed completion rates for Control and Test groups\n",
    "p_control_observed = completion_rates['Control']\n",
    "p_test_observed = completion_rates['Test']\n",
    "\n",
    "# Calculate the observed difference in completion rates\n",
    "observed_difference = p_test_observed - p_control_observed\n",
    "\n",
    "# Check if the observed difference meets the 5% threshold\n",
    "meets_threshold = observed_difference >= 0.05\n",
    "\n",
    "observed_difference, meets_threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f27066c-9500-43f9-bcf6-e166a5dceec0",
   "metadata": {},
   "source": [
    "The observed difference in completion rates between the Test group (with the new design) and the Control group (with the old design) is approximately 21%, which is substantially higher than Vanguard's threshold of 5%.\n",
    "\n",
    "However, we should still test if this 21% difference is statistically significant. If it is, then while the new design might not be justifiable purely from a cost perspective, it still represents a genuine improvement.\n",
    "\n",
    "Let's conduct the two-proportion z-test again to determine the statistical significance of the observed difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ae2216-2c92-480a-be09-1c92dab510c9",
   "metadata": {},
   "source": [
    "#### 1. **State the Hypothesis**\n",
    "\n",
    "Given the goal of the experiment and the 5% threshold:\n",
    "\n",
    "**Null Hypothesis ($(H_0$)):** The completion rate for the Test group (new design) is equal to or less than the completion rate for the Control group (old design) increased by 5%.\n",
    "\n",
    "**Alternative Hypothesis ($(H_a$)):** The completion rate for the Test group (new design) is greater than the completion rate for the Control group (old design) increased by 5%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb69d7-971d-4904-8369-6e70d6549eb5",
   "metadata": {},
   "source": [
    "#### 2. **Choose the Right Statistical Test**\n",
    "\n",
    "Given that we are comparing proportions between two groups, a **one-sided** two-proportion z-test is appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18ddff-0cb4-4f3f-9dec-c1ac6d6571d2",
   "metadata": {},
   "source": [
    "#### 3. **Check for Assumptions**\n",
    "\n",
    "This is the same as what we did in the test above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e7f884-4aa6-45ec-8aa3-3d05fe0ad10e",
   "metadata": {},
   "source": [
    "\n",
    "#### 4. **Conduct the Test**\n",
    "\n",
    "We've already performed the test earlier, but given the new threshold, we'll compare the completion rate of the Test group to the completion rate of the Control group increased by 5%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b91a2-419d-43d3-84a2-b43aa6c5f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the previous test, we did\n",
    "# Number of successes (completions) for each group\n",
    "count = df_confirmations['Variation'].value_counts()\n",
    "\n",
    "# Number of trials (total users) for each group\n",
    "nobs = df_experiment_clients['Variation'].value_counts()\n",
    "\n",
    "# Calculate the adjusted number of successes for Control\n",
    "# Adjust the control proportion by adding the 5% threshold\n",
    "adjusted_p_control = completion_rates['Control'] + 0.05\n",
    "n_control = df_experiment_clients['Variation'].value_counts()['Control']\n",
    "count_adjusted = [count[0], int(adjusted_p_control * n_control)]\n",
    "\n",
    "# Conduct the two-proportion z-test using the adjusted control count\n",
    "z_stat_adjusted, p_value_adjusted = proportions_ztest(count_adjusted, nobs, alternative='larger')  # one-sided test\n",
    "\n",
    "z_stat_adjusted, p_value_adjusted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f76b3c-0344-42f5-a2cc-c770a70eb60b",
   "metadata": {},
   "source": [
    "#### 5. **Interpret Results and Decision**\n",
    "\n",
    "- Given the extremely low p-value (essentially 0) and the nature of the test being one-sided, we can reject the null hypothesis ($(H_0$)) at any conventional significance level (e.g., $( \\alpha = 0.05 $)). This confirms there's statistically significant evidence to suggest that the completion rate for the Test group (with the new design) is greater than the completion rate for the Control group (with the old design) increased by 5%.\n",
    "\n",
    "- Given the statistically significant improvement in the completion rate and the fact that this improvement meets Vanguard's cost-effectiveness threshold, Vanguard **should implement the new UI design** as it not only has proven to be statistically better but also meets the practical significance set by the company in terms of cost justification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8382df9d-01ac-45c2-b3ff-275bd7d00757",
   "metadata": {},
   "source": [
    "### Other Hypothesis Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcbeb85-fb93-43a8-babe-5f068f99c2f2",
   "metadata": {},
   "source": [
    "Choose other hypothesis you want to test. For example, you might want to test whether the average age of clients engaging with the new process is the same as those engaging with the old process; or if the average client tenure (how long they've been with Vanguard) of those engaging with the new process is the same as those engaging with the old process; or if there are gender differences that affect engaging with the new or old process etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027fea98-4e19-48c1-b3ff-c1df39aadbf4",
   "metadata": {},
   "source": [
    "#### Average age of clients\n",
    "\n",
    "Let's test the hypothesis regarding the **average age of clients** between the two groups (Test and Control)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f220c6c-3bff-437a-97ee-300242ed0cfa",
   "metadata": {},
   "source": [
    "##### **Hypothesis**\n",
    "\n",
    "1. **Null Hypothesis $(H_0$)**: The average age of clients for the Test group (new design) is equal to the average age of clients for the Control group (old design).\n",
    "2. **Alternative Hypothesis $(H_a$)**: The average age of clients for the Test group (new design) is not equal to the average age of clients for the Control group (old design)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f9c8a-ddd7-4717-b896-a5fcb7149e16",
   "metadata": {},
   "source": [
    "##### Statistical Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf6e03-1dab-40d3-9aa9-4e901ef1dedf",
   "metadata": {},
   "source": [
    "To test this hypothesis, we will use a two-sample t-test, which is appropriate for comparing means between two independent groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aff616-e12f-46d2-a998-d003030f6827",
   "metadata": {},
   "source": [
    "##### Test assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12e3b1-0f31-48b5-b741-e2d245c78fff",
   "metadata": {},
   "source": [
    "The assumptions for the independent two-sample t-test (which we will apply here) include:\n",
    "\n",
    "1. **Independence of Observations**: The two groups (Test and Control) are independent of each other.\n",
    "2. **Normality**: The dependent variable should be approximately normally distributed in each group. This can be checked using plots (like Q-Q plots) or tests (like the Shapiro-Wilk test).\n",
    "3. **Homogeneity of Variances**: The variances of the dependent variable should be equal in the two groups. This can be checked using Levene's test. If the variances are not equal, we can still conduct the t-test by adjusting for unequal variances, using the `equal_var=False` parameter.\n",
    "\n",
    "Let's check the assumptions of normality and homogeneity of variances.\n",
    "\n",
    "\n",
    "1. **Normality Assumption**:\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b305b281-e777-4ce2-bc6b-b2050e722c54",
   "metadata": {},
   "source": [
    "It's worth noting that the t-test is robust against this assumption when sample sizes are large, as in our case.\n",
    "If we didn't have a large sample size (N > 5000) we would run a Shapiro test as follows:\n",
    "```python\n",
    "# Check for normality using Shapiro-Wilk test for both groups\n",
    "shapiro_test = stats.shapiro(ages_test)\n",
    "shapiro_control = stats.shapiro(ages_control)\n",
    "\n",
    "shapiro_test, shapiro_control\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b384ba5-465b-4d52-a062-78dfef89e6f7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "2. **Homogeneity of Variances Assumption**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dffc61a-a55d-4738-b932-9c5acb69bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Now merge the data to get the age of clients for each variation\n",
    "df_full_merged = pd.merge(df_web_experiment_merged, df_demo, on='client_id', how='inner')\n",
    "\n",
    "# Extract the client ages for the Test and Control groups\n",
    "ages_test = df_full_merged[df_full_merged['Variation'] == 'Test']['clnt_age']\n",
    "ages_control = df_full_merged[df_full_merged['Variation'] == 'Control']['clnt_age']\n",
    "\n",
    "# Check for homogeneity of variances using Levene's test\n",
    "levene_result = stats.levene(ages_test, ages_control)\n",
    "levene_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db95b60c-b6d4-4913-bbe1-e96148e1ba97",
   "metadata": {},
   "source": [
    "   - Levene's test provides a p-value of \\(0.71\\), meaning we fail to reject the null hypothesis. Thus, we can assume that the variances between the two groups are approximately equal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d281a-9eb6-4e9c-a341-d26713c3ecf7",
   "metadata": {},
   "source": [
    "Given these results:\n",
    "- The t-test is known to be robust against violations of normality assumption, especially with large sample sizes.\n",
    "- We have met the assumption of equal variances.\n",
    "\n",
    "Considering the large sample size and the central limit theorem (which suggests that the sampling distribution of the mean will be approximately normal for large samples regardless of the underlying distribution), our t-test results are still valid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953c977e-f205-41fc-a775-4f391d0dc0de",
   "metadata": {},
   "source": [
    "##### Run test, interpret results and make a decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e47d7b-9ad2-4b19-93a5-64317f6158f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ages for Test and Control groups using the correct column name\n",
    "ages_test = df_full_merged[df_full_merged['Variation'] == 'Test']['clnt_age'].dropna()\n",
    "ages_control = df_full_merged[df_full_merged['Variation'] == 'Control']['clnt_age'].dropna()\n",
    "\n",
    "# Conduct a two-sample t-test\n",
    "t_stat, p_value_age = stats.ttest_ind(ages_test, ages_control, equal_var=False)  # Assuming unequal variances\n",
    "\n",
    "t_stat, p_value_age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480e32aa-9eef-4f58-ae06-c62d0a83b9d7",
   "metadata": {},
   "source": [
    "The results of the two-sample t-test for the average age of clients engaging with the new design versus the old design are as follows:\n",
    "\n",
    "- **T-Statistic**: 7.83\n",
    "- **P-Value**: $(4.71 \\times 10^{-15}$)\n",
    "\n",
    "Given the extremely low p-value, we can reject the null hypothesis ($(H_0$)) at any conventional significance level (e.g., $( \\alpha = 0.05 $)). This means there is statistically significant evidence to suggest that the average age of clients engaging with the new design (Test group) is different from those engaging with the old design (Control group).\n",
    "\n",
    "The positive t-statistic indicates that the average age of clients in the Test group is higher than that in the Control group.\n",
    "\n",
    "Therefore, this analysis suggests that the new design might be more appealing or user-friendly to an older demographic compared to the old design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4778354-0c8b-4d67-bc31-d1cb561f98c5",
   "metadata": {},
   "source": [
    "**Decision**:\n",
    "\n",
    "If Vanguard's goal was to make the new design more attractive to an older demographic, then this is a positive outcome. However, if the aim was a universal appeal or targeting a younger demographic, then the design might need further adjustments. The next steps would depend on Vanguard's business objectives and target audience for the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79646d8-75d9-492b-b9a3-38ae007dcd5c",
   "metadata": {},
   "source": [
    "#### Average client tenure\n",
    "\n",
    "Let's explore the hypothesis regarding the **average client tenure** between the two groups (Test and Control). This can provide insights into whether newer clients react differently to the design compared to longer-standing clients. \n",
    "\n",
    "**Hypothesis:**\n",
    "1. **Null Hypothesis ($H_0$)**: The average client tenure for the Test group (new design) is equal to the average client tenure for the Control group (old design).\n",
    "2. **Alternative Hypothesis ($H_a$)**: The average client tenure for the Test group (new design) is not equal to the average client tenure for the Control group (old design).\n",
    "\n",
    "To test this hypothesis, we will use a two-sample t-test since we are comparing the means of two independent groups. \n",
    "\n",
    "Let's proceed with this analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724cde79-9df1-4d6a-9ec8-5f1c6f45ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average client tenure (in years) for both Test and Control groups\n",
    "avg_tenure_summary = df_demo_experiment_merged.groupby('Variation')['clnt_tenure_yr'].mean()\n",
    "\n",
    "avg_tenure_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b352bc5e-5d09-4e65-ad92-bfebebd67673",
   "metadata": {},
   "source": [
    "Now, we'll conduct the two-sample t-test to compare the means of the client tenures between the Test and Control groups. This test will help us determine if there's a statistically significant difference between the average client tenures of the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ee7d1-0899-4c32-8518-2dc7b3bac649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Extract the client tenures for the Test and Control groups\n",
    "test_tenure = df_demo_experiment_merged[df_demo_experiment_merged['Variation'] == 'Test']['clnt_tenure_yr']\n",
    "control_tenure = df_demo_experiment_merged[df_demo_experiment_merged['Variation'] == 'Control']['clnt_tenure_yr']\n",
    "\n",
    "# Conduct the two-sample t-test\n",
    "t_stat, p_value = ttest_ind(test_tenure, control_tenure)\n",
    "\n",
    "t_stat, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fdbfdd-a42e-4d69-b245-5be618ff0823",
   "metadata": {},
   "source": [
    "The results of the two-sample t-test are as follows:\n",
    "\n",
    "- $ t $-statistic: -1.7121\n",
    "- $ p $-value: 0.0869\n",
    "\n",
    "Given the $ p $-value of 0.0869, which is greater than the typical significance level of 0.05, we fail to reject the null hypothesis $( H_0 $). This means that we don't have enough statistical evidence to claim that the average client tenure for the Test group is different from that of the Control group.\n",
    "\n",
    "In conclusion, based on this test, there's no significant difference in the average client tenure between the Test and Control groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc1e479-3361-467c-b54e-e8ae0e5ab747",
   "metadata": {},
   "source": [
    "#### Gender Differences in Engaging with the New or Old Process\n",
    "\n",
    "Let's test a different hypothesis: **Gender Differences in Engaging with the New or Old Process**.\n",
    "\n",
    "**Hypothesis:**\n",
    "1. **Null Hypothesis $(H_0$)**: The proportion of male clients engaging with the new design is the same as the proportion of male clients engaging with the old design.\n",
    "2. **Alternative Hypothesis $(H_a$)**: The proportion of male clients engaging with the new design is different from the proportion of male clients engaging with the old design.\n",
    "We'll proceed with this analysis, looking at gender differences between the two groups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771ee2d-7506-4f7b-93e0-3804d16d5fcb",
   "metadata": {},
   "source": [
    "To test the given hypothesis about gender differences between the two groups, we'll be using a test of proportions. Specifically, we can use the two-proportion z-test.\n",
    "\n",
    "Here's the plan:\n",
    "\n",
    "1. Extract the number of female clients in the Test group and the Control group.\n",
    "2. Extract the total number of clients in both groups.\n",
    "3. Use the two-proportion z-test to compare the proportions.\n",
    "\n",
    "Let's start by extracting the necessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c6d47-9d34-4198-a380-4214969c3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the number of female clients in the Test and Control groups\n",
    "female_test = df_demo_experiment_merged[(df_demo_experiment_merged['Variation'] == 'Test') & (df_demo_experiment_merged['gendr'] == 'F')].shape[0]\n",
    "female_control = df_demo_experiment_merged[(df_demo_experiment_merged['Variation'] == 'Control') & (df_demo_experiment_merged['gendr'] == 'F')].shape[0]\n",
    "\n",
    "# Extract the total number of clients in the Test and Control groups\n",
    "total_test = df_demo_experiment_merged[df_demo_experiment_merged['Variation'] == 'Test'].shape[0]\n",
    "total_control = df_demo_experiment_merged[df_demo_experiment_merged['Variation'] == 'Control'].shape[0]\n",
    "\n",
    "female_test, female_control, total_test, total_control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79b55a-3a63-4eae-8c78-6019f644ae0b",
   "metadata": {},
   "source": [
    "Now, let's perform the two-proportion z-test to compare the proportions of female clients engaging with the new design versus the old design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d44e8-8d7c-4a7e-a730-b3c2a558e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# Define the counts of successes (female clients) and the total number of trials\n",
    "count = [female_test, female_control]\n",
    "nobs = [total_test, total_control]\n",
    "\n",
    "# Perform the two-proportion z-test\n",
    "z_stat, p_value = proportions_ztest(count, nobs)\n",
    "\n",
    "z_stat, p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b9e86-1934-47e8-9561-61feb2737886",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The results of the two-proportion z-test are as follows:\n",
    "\n",
    "- $ z $-statistic: 0.63\n",
    "- $ p $-value: 0.52\n",
    "\n",
    "Given the $ p $-value of 0.1677, which is greater than the typical significance level of 0.05, we fail to reject the null hypothesis $( H_0 $). This means that we don't have enough statistical evidence to claim that the proportion of female clients engaging with the new design is different from the proportion of female clients engaging with the old design.\n",
    "\n",
    "In conclusion, based on this test, there's no significant gender difference in terms of engaging with the new or old process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276acb8d-3593-4119-85d1-56cc361bd6b2",
   "metadata": {},
   "source": [
    "### **4. Experiment Evaluation**\n",
    "\n",
    "- **Design Effectiveness**:\n",
    "    - Was the experiment well-structured? Were clients randomly and equally divided between the old and new designs? Were there any biases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340c7840-cc30-49cd-9aa9-a5d161628a03",
   "metadata": {},
   "source": [
    "To evaluate the design effectiveness and ensure that the experiment was well-structured, we need to address the following points:\n",
    "\n",
    "1. **Random Assignment**: Clients should be randomly assigned to either the Test or Control group to ensure that the groups are comparable at the start of the experiment.\n",
    "2. **Group Sizes**: The groups should be roughly equal in size or at least have enough samples to achieve sufficient statistical power. A substantial imbalance might raise concerns about the random assignment process.\n",
    "3. **Baseline Comparability**: We should check if the two groups were comparable in terms of key metrics (like age, gender, tenure, etc.) before the intervention. Any significant differences could indicate biases in group assignment.\n",
    "4. **Other Biases**: We need to identify any other potential biases that could affect the experiment's outcome, such as the time of year the experiment was conducted, external events, etc.\n",
    "\n",
    "Let's start by examining the group sizes and then check the baseline comparability between the Test and Control groups in terms of age, gender, and tenure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571b6be1-d311-4698-bd63-aef775319c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = df_demo_experiment_merged.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43238cc-b4fd-43bb-9a64-4a17988b22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of clients in Test and Control groups\n",
    "group_distribution = merged_data['Variation'].value_counts()\n",
    "\n",
    "# Checking baseline comparability in terms of age, gender, and tenure\n",
    "mean_age_test = merged_data[merged_data['Variation'] == 'Test']['clnt_age'].mean()\n",
    "mean_age_control = merged_data[merged_data['Variation'] == 'Control']['clnt_age'].mean()\n",
    "\n",
    "male_prop_test = male_test / total_test\n",
    "male_prop_control = male_control / total_control\n",
    "\n",
    "mean_tenure_test = merged_data[merged_data['Variation'] == 'Test']['clnt_tenure_yr'].mean()\n",
    "mean_tenure_control = merged_data[merged_data['Variation'] == 'Control']['clnt_tenure_yr'].mean()\n",
    "\n",
    "group_distribution, (mean_age_test, mean_age_control), (male_prop_test, male_prop_control), (mean_tenure_test, mean_tenure_control)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e6edad-e138-44d9-aa8a-ece41d4b6ab6",
   "metadata": {},
   "source": [
    "Here are the findings regarding the design effectiveness:\n",
    "\n",
    "1. **Group Sizes**:\n",
    "   - Test group: 26,968 clients\n",
    "   - Control group: 23,532 clients\n",
    "\n",
    "   The groups are not of equal size, but they are sufficiently large, and the imbalance is not substantial.\n",
    "\n",
    "2. **Baseline Comparability**:\n",
    "   - **Age**:\n",
    "     - Test group: Average age is 47.16 years\n",
    "     - Control group: Average age is 47.50 years\n",
    "     \n",
    "   The average ages are quite close between the two groups.\n",
    "   \n",
    "   - **Gender**:\n",
    "     - Test group: 33.29% are male\n",
    "     - Control group: 33.87% are male\n",
    "     \n",
    "   The proportions of male clients are also close between the two groups.\n",
    "   \n",
    "   - **Tenure**:\n",
    "     - Test group: Average tenure is 11.98 years\n",
    "     - Control group: Average tenure is 12.09 years\n",
    "     \n",
    "   The average tenures are quite close between the two groups.\n",
    "\n",
    "3. **Other Biases**:\n",
    "   - We have not specifically checked for other biases like the time of year the experiment was conducted, external events, etc. Further information or additional data would be required to evaluate these potential biases.\n",
    "\n",
    "In conclusion, based on the information we have, the experiment appears to be well-structured. The groups are comparable in terms of key metrics at the beginning of the experiment, indicating that clients were likely randomly assigned. The slight size difference between the Test and Control groups is not a major concern given the large sample sizes. However, to completely rule out any biases, we would need more context or additional data on other potential factors that could influence the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a61d1c-3e4c-47e2-9c16-bc82e6e9bb88",
   "metadata": {},
   "source": [
    "- **Duration Assessment**:\n",
    "    - Was the timeframe of the experiment adequate to gather meaningful data and insights?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b41f8e-a436-415a-b37e-61a75545feb3",
   "metadata": {},
   "source": [
    "To assess whether the timeframe of the experiment was adequate, we'll consider the following factors:\n",
    "\n",
    "1. **Duration of Experiment**: We should first determine the exact duration of the experiment in terms of days.\n",
    "2. **Volume of Data**: A longer experiment duration might be necessary if there is low client activity, and a shorter duration might suffice if there's high activity.\n",
    "3. **Variability Over Time**: If there are weekly or monthly patterns or any other seasonality in client engagement, a longer duration would capture those patterns.\n",
    "4. **External Factors**: Were there any external events or factors that could have influenced the results during this period? For instance, holidays, marketing campaigns, or other significant events could impact user behavior.\n",
    "5. **Stability of Metrics**: If the metrics being observed stabilize quickly and remain consistent, a shorter experiment might be sufficient. Conversely, if they fluctuate significantly, a longer timeframe might be necessary.\n",
    "\n",
    "Let's start by determining the exact duration of the experiment and analyzing the volume of data during this period. We'll then assess the variability over time by looking at the web data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2aec6d-3bfb-4938-b221-8dc15043bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date_time column to datetime format\n",
    "df_web_data['date_time'] = pd.to_datetime(df_web_data['date_time'])\n",
    "\n",
    "# Determine the exact duration of the experiment\n",
    "start_date = df_web_data['date_time'].min()\n",
    "end_date = df_web_data['date_time'].max()\n",
    "duration = (end_date - start_date).days\n",
    "\n",
    "# Analyze the volume of data (number of engagements) for each day of the experiment\n",
    "daily_engagements = df_web_data.groupby(df_web_data['date_time'].dt.date).size()\n",
    "\n",
    "duration, daily_engagements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9eea60-2251-4e26-87e9-0937d1f83667",
   "metadata": {},
   "source": [
    "Here's the assessment based on the data:\n",
    "\n",
    "1. **Duration of Experiment**: \n",
    "   - The experiment ran for 97 days, from 2017-03-15 to 2017-06-20.\n",
    "\n",
    "2. **Volume of Data**: \n",
    "   - The daily engagements (number of interactions) range from a few hundred to several thousand per day. This indicates a high volume of data, which is good for capturing meaningful insights.\n",
    "\n",
    "3. **Variability Over Time**: \n",
    "   - The volume of data varies from day to day, and there's no obvious pattern at a glance. However, it's worth noting that the engagements increased significantly towards the end of the experiment period. This could be influenced by factors like marketing campaigns, seasonality, or external events.\n",
    "\n",
    "To further assess the duration's adequacy, we can visualize the daily engagements to see if there are any clear patterns or anomalies. This will help determine if the experiment captured any weekly or monthly trends and if the timeframe was sufficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875ebae1-8d41-497b-9d9a-fcc02d0e473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting daily engagements over time\n",
    "plt.figure(figsize=(15, 7))\n",
    "daily_engagements.plot()\n",
    "plt.title('Daily Engagements Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Engagements')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a5e9e-198f-42ac-917d-35ca32ae8748",
   "metadata": {},
   "source": [
    "The visualization presents the daily engagements over the course of the experiment. \n",
    "\n",
    "Here are some observations:\n",
    "\n",
    "1. **Trends**: There's a noticeable increase in engagements, with a subsequent decline followed by another surge. This suggests some variability and potentially external influences affecting client engagement.\n",
    "2. **Weekly Patterns**: There seems to be some periodicity in the data, possibly indicating weekly patterns. This periodicity is a good sign as it suggests that the duration of the experiment was long enough to capture these patterns multiple times.\n",
    "\n",
    "In conclusion:\n",
    "\n",
    "- The experiment's duration of 97 days seems adequate to capture a variety of patterns and trends.\n",
    "- The experiment spanned multiple weeks, which is beneficial for capturing any weekly patterns.\n",
    "\n",
    "It might be beneficial to investigate the reasons behind the significant spikes in engagements and ensure they aren't introducing any biases to the experimental results. Overall, the timeframe appears sufficient to gather meaningful insights, but understanding the context behind significant changes in engagement is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5323b2-5299-4e9a-ae33-e00c84456202",
   "metadata": {},
   "source": [
    "- **Additional Data Needs**:\n",
    "    - What other data, if available, could enhance the analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c127b-a64a-4775-9b9b-aa48ca5dd458",
   "metadata": {},
   "source": [
    "Having additional data can provide a more comprehensive view of the experiment and aid in drawing more accurate and insightful conclusions. Here are some types of data that could enhance the analysis:\n",
    "\n",
    "1. **User Feedback**: Direct feedback from clients about their experience with the new vs. old design can provide qualitative insights into the quantitative findings.\n",
    "  \n",
    "2. **Engagement Duration**: The amount of time clients spend on the platform can provide insights into their level of engagement and satisfaction.\n",
    "  \n",
    "3. **Session Details**: Information about the specific actions users take during each session (e.g., pages visited, features used) can highlight which aspects of the design are most and least effective.\n",
    "  \n",
    "4. **Conversion Metrics**: If there are specific actions or outcomes the design aims to promote (e.g., product purchases, sign-ups), tracking these conversion metrics can be invaluable.\n",
    "  \n",
    "5. **Device and Browser Information**: Understanding which devices (mobile vs. desktop) or browsers clients are using can reveal if there are design inconsistencies or issues specific to certain technologies.\n",
    "  \n",
    "6. **External Factors**: Data on external marketing campaigns, promotions, or events that occurred during the experiment period can help explain spikes or drops in engagement.\n",
    "  \n",
    "7. **Demographic Segmentation**: More detailed demographic or behavioral data (e.g., occupation, education level, frequency of use) can help in segmenting the analysis and understanding how different client groups react to the designs.\n",
    "  \n",
    "8. **Error Logs**: If there were technical issues or bugs with either design, error logs can provide insights into how often these issues occurred and their impact.\n",
    "  \n",
    "9. **Exit Surveys**: Surveys given to clients who decide to stop using the service during the experiment period can provide insights into potential issues with either design.\n",
    "  \n",
    "10. **Historical Data**: Data from before the experiment started can set a baseline and help understand if observed changes are truly due to the experiment or are part of a larger trend.\n",
    "\n",
    "While not all additional data types may be relevant or feasible for every experiment, considering these can lead to a more holistic understanding of the experiment's results and the factors driving those results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b12a3-966f-420f-9bae-b2fb0633b23c",
   "metadata": {},
   "source": [
    "# Bonus: More on Client Behavior Analysis\n",
    "\n",
    "\n",
    "## Interaction Patterns\n",
    "    - How do clients navigate through the old versus the new digital process? Do they follow similar steps or diverge at certain points?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ee8bb2-5fef-4e03-8d25-85484b9a34b7",
   "metadata": {},
   "source": [
    "To analyze how clients navigate the old vs. new digital process, we'll need to:\n",
    "\n",
    "- Join the **df_web_data** with **df_experiment_clients** to determine which process each navigation step belongs to.\n",
    "- Analyze the frequency and order of the process steps for both \"Control\" and \"Test\" groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd098484-6b63-4225-8972-44a3503d1100",
   "metadata": {},
   "source": [
    "Let's start by merging the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c656ac-df45-470f-9330-73b003268bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first few rows of the merged dataframe\n",
    "df_web_experiment_merged.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb1a34-713f-4fd3-aed3-1b46bbac6f0f",
   "metadata": {},
   "source": [
    "The datasets have been successfully merged. The `Variation` column indicates whether a particular navigation step is from the old process (\"Control\") or the new one (\"Test\").\n",
    "\n",
    "To understand how clients navigate through the old and new digital processes, we'll:\n",
    "\n",
    "1. Group by the `Variation` and `process_step` columns to determine the frequency of each step for both the old and new processes.\n",
    "2. Analyze the order in which the steps are taken for each process.\n",
    "\n",
    "Let's start by analyzing the frequency of each step for both processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f3ade-7455-4b17-900a-b95553dce121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by Variation and process_step to get the frequency of each step\n",
    "step_frequency = df_web_experiment_merged.groupby(['Variation', 'process_step']).size().reset_index(name='frequency')\n",
    "\n",
    "# Sorting the frequencies for better visualization\n",
    "step_frequency = step_frequency.sort_values(by=['Variation', 'frequency'], ascending=[True, False])\n",
    "\n",
    "step_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6a736-fb23-4d87-bc3e-9586838f1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of seeing this result, in a pivot table\n",
    "\n",
    "# Group by Variation and process_step to get the frequency of each step for both processes\n",
    "step_frequency = df_web_experiment_merged.groupby(['Variation', 'process_step']).size().reset_index(name='count')\n",
    "\n",
    "step_frequency.pivot(index='process_step', columns='Variation', values='count').fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cde7e5-b4d4-4849-a1b9-13dcd2c46de2",
   "metadata": {},
   "source": [
    "From the frequency analysis, we observe the following:\n",
    "\n",
    "- For both the old (Control) and new (Test) digital processes, the most common first step is \"start\", with more clients starting the process in the Test group compared to the Control group.\n",
    "- The sequence generally seems to be \"start\" -> \"step_1\" -> \"step_2\" -> \"step_3\" -> \"confirm\", based on the decreasing counts.\n",
    "- For every step in the process, there are more occurrences in the Test group than in the Control group, suggesting that the new digital process might be more engaging or user-friendly, leading to more clients reaching each subsequent step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e155f7f6-a7a7-4782-a732-5e35956159db",
   "metadata": {},
   "source": [
    "## Difference in the number of actions (steps) taken \n",
    "\n",
    "**Objective**:\n",
    "\n",
    "We aim to investigate if there's a difference in the number of actions (steps) taken by users between the Test and Control groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607fb76-7879-4a0e-985e-b89ba60d54c6",
   "metadata": {},
   "source": [
    "**Hypothesis:**\n",
    "1. **Null Hypothesis $(H_0$)**: The average number of actions taken by users for the Test group (new design) is equal to the average number of actions taken by users for the Control group (old design).\n",
    "2. **Alternative Hypothesis $(H_a$)**: The average number of actions taken by users for the Test group (new design) is different from the average number of actions taken by users for the Control group (old design).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52cb37-e75c-4baa-bb36-f56c91ea342e",
   "metadata": {},
   "source": [
    "**Data Preparation**\n",
    "\n",
    "To test this hypothesis, we'll need to:\n",
    "\n",
    "\n",
    "1. Aggregate the number of actions (steps) taken by each user in both the Test and Control groups.\n",
    "1. Compare the means to understand the data.\n",
    "2. Use a two-sample t-test to compare the means of the two groups.\n",
    "\n",
    "Let's begin by aggregating the number of actions taken by each user in the Test and Control groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972ea77-23fc-41fe-86eb-f6181407df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the relevant columns: client_id, Variation, process_step\n",
    "data = df_web_experiment_merged[['client_id', 'Variation', 'process_step']]\n",
    "\n",
    "# Group by client_id and Variation and count the number of process steps (actions)\n",
    "grouped_data = data.groupby(['client_id', 'Variation']).count().reset_index()\n",
    "\n",
    "# Split the data into control and test groups\n",
    "control_group = grouped_data[grouped_data['Variation'] == 'Control']\n",
    "test_group = grouped_data[grouped_data['Variation'] == 'Test']\n",
    "\n",
    "control_group.head(), test_group.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3f5021-58ad-4a97-9ae0-74a3527137f5",
   "metadata": {},
   "source": [
    "Before conducting the hypothesis test, let's get an initial sense of the data by comparing the means of the two groups. This will give us an idea about which group tends to take more actions on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7422a2-6850-46c5-8aa7-0c7b3a06ef27",
   "metadata": {},
   "source": [
    "To determine which group takes more actions on average, you can compare the mean number of actions (process steps) for the Control group with that of the Test group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0dc5ce-2218-468d-a13c-22eb14cf550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean number of actions for both groups\n",
    "mean_control = control_group['process_step'].mean()\n",
    "mean_test = test_group['process_step'].mean()\n",
    "\n",
    "mean_control, mean_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3336955-956f-47fc-9b2a-b41259865fa3",
   "metadata": {},
   "source": [
    "From this initial comparison, we observe that users in the Test group (with the new design) tend to take more actions on average compared to users in the Control group (with the old design)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a08d11-627f-46c3-beb9-889055a88ad8",
   "metadata": {},
   "source": [
    "To determine if there's a significant difference in the average number of actions taken by users between the Test and Control groups, we'll perform an independent two-sample t-test. This test will compare the means of the two groups to check if they are statistically different from each other.\n",
    "\n",
    "Let's conduct the t-test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5de477-d1c3-4ac0-9982-88edfd364f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Conduct an independent t-test\n",
    "t_stat, p_value = ttest_ind(control_group['process_step'], test_group['process_step'])\n",
    "\n",
    "t_stat, p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f6c1d-cc6c-4784-9d47-67154cecd78f",
   "metadata": {},
   "source": [
    "Given the very small p-value (much less than the typical significance level of 0.05), we can reject the null hypothesis. This suggests that there is a statistically significant difference in the average number of actions taken by users between the Test and Control groups.\n",
    "\n",
    "**Conclusion**:\n",
    "\n",
    "This suggests that there is a statistically significant difference in the average number of actions taken by users between the Test and Control groups. The data provides strong evidence to support the alternative hypothesis that the average number of actions taken by users for the Test group (new design) is different from the average number of actions taken by users for the Control group (old design). Furthermore, as observed from the means, users in the Test group tend to take more actions on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7526c28-a632-495f-a358-81028d84fb1b",
   "metadata": {},
   "source": [
    "# Bonus - power and effect size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381490e3-64ee-4f2e-9c55-66bcf8320632",
   "metadata": {},
   "source": [
    "## Power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea56b71a-4f97-42ab-a2b9-edf8bfa42594",
   "metadata": {},
   "source": [
    "\n",
    "**Definition**: Power is the probability that a statistical test correctly rejects the null hypothesis when it is indeed false. In simpler terms, it's the ability of a test to detect an effect if there truly is one.\n",
    "- It's the complement of the Type II error rate ($beta$): ($1 - \\beta$). A common value for power is 1–0.2 = 0.8. \n",
    "\n",
    "**Importance**: \n",
    "- Imagine you're trying to find out if a new medicine works better than an old one. If the test has low power, you might conclude that the new medicine doesn't make a difference, when in reality, it does.\n",
    "- High power reduces the risk of Type II errors (failing to detect an effect when one exists).\n",
    "\n",
    "**Designing an Experiment vs. Interpreting Results**:\n",
    "- **Designing**: Before running an experiment, researchers often conduct a \"power analysis\" to determine the necessary sample size to detect an effect of a certain size with a certain degree of confidence. If power is too low, the researcher may increase the sample size or adjust the design to improve the power.\n",
    "    -    - In this context, you usually know or have an estimate of the effect size you care about (based on prior studies, expert judgment, or practical significance), and you want to ensure your study is sufficiently powered to detect this effect.\n",
    "- **Interpreting**: After the results are in, power (also Post-hoc Power) can provide context. Its used especially when an expected effect was not found (i.e., a non-significant result).\n",
    "   - The primary purpose is to determine if the study was underpowered (i.e., the sample size was too small) to detect an effect of the observed size.\n",
    "   - This can help differentiate between two interpretations of a non-significant result: \n",
    "     1. The true effect is close to zero.\n",
    "     2. The study was underpowered to detect the true effect.\n",
    "   - However, post-hoc power analysis has been criticized in the statistical community because it can be redundant. For example, if you have a non-significant result, post-hoc power will inevitably be low. If you have a significant result, post-hoc power will be high.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5b8571-3e5a-47fa-a626-45941a41f8ec",
   "metadata": {},
   "source": [
    "## **Effect Size**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd238234-f71d-4b2b-bfbb-f9aae0ecefbb",
   "metadata": {},
   "source": [
    "**Definition**: Effect size quantifies the size of the difference between groups or the strength of a relationship between variables. It provides a measure that is free from sample size, allowing a comparison of results across different studies or experiments.\n",
    "- Common measurement is Cohen’s h or d. Cohen’s d is used for comparison between 2 means and Cohen’s h is used for comparison between 2 proportions. \n",
    "- In cases where effect size is unknown, an accepted benchmark set by Cohen as a rule of thumb for effect size is as follows: Small = 0.2, Medium = 0.5, Large = 0.8. \n",
    "- This is set based on the experiment’s unique context on how much of an effect from the treatment will be considered as great/significant for the company.\n",
    "\n",
    "**Importance**:\n",
    "- Small sample studies can produce statistically significant results even for trivial findings, while large sample studies might find statistically insignificant results that are still of practical significance. Effect size helps in distinguishing between statistical significance and \"real-world\" or practical significance.\n",
    "\n",
    "**Designing an Experiment vs. Interpreting Results**:\n",
    "- **Designing**: Researchers make an educated guess about the expected effect size based on previous research or pilot studies. This anticipated effect size is then used in power analysis to determine the required sample size.\n",
    "- **Interpreting**: Once the study is done, the calculated effect size tells us how large the observed effect is. Coupled with statistical significance, it provides a fuller picture of the results. For instance, you might have a significant result, but if the effect size is tiny, it might not be of practical importance.\n",
    "\n",
    "**In Summary**\n",
    "\n",
    "While p-values tell us if an effect exists, the power and effect size tell us how confident we should be in that result and how big that effect is, respectively. Power and effect size are complementary tools to p-values and are crucial for both designing robust experiments and interpreting their results in a meaningful context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007722ad-7ad1-4124-8346-173b4e548f66",
   "metadata": {},
   "source": [
    "## **Interpreting results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5556a5-0eae-4e01-b17c-0f8da221d43d",
   "metadata": {},
   "source": [
    "Lets calculate the effect size and post-hoc power for the first test: since the new design (Test group) had a higher completion rate compared to the old design (Control group), we might be interested in confirming if this difference is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ae2c7-b023-44dd-b169-2cba45d0afc7",
   "metadata": {},
   "source": [
    "1. **Effect Size Calculation**:\n",
    "   We'll use Cohen's $h$ for the effect size in a two-proportions scenario. The formula for Cohen's $h$ is:\n",
    "   \n",
    "   $$[ h = 2 \\times (\\arcsin(\\sqrt{p_1}) - \\arcsin(\\sqrt{p_2})) ]$$\n",
    "\n",
    "   where $p_1$ and $p_2$ are the proportions from the two groups.\n",
    "\n",
    "2. **Power Calculation** (post-hoc power):\n",
    "   We'll use the `statsmodels` library, which provides a function to calculate the power of a two-proportions z-test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae018f39-129e-4892-8e51-388573288cc8",
   "metadata": {},
   "source": [
    "With these, we can determine the power of the test, which tells us the probability that we would detect a difference in completion rates given our sample sizes and the observed effect size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3380e8f-1c86-465e-80f1-3b94d0a37c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "# grouped_data calculated above has total_users which is n (control and test), and p (control and test)\n",
    "n_control = grouped_data.total_users[0] # n control\n",
    "n_test = grouped_data.total_users[1] # n test\n",
    "p_control = grouped_data.p[0] # p control\n",
    "p_test = grouped_data.p[1] # p test\n",
    "\n",
    "# Calculate Cohen's h as effect size\n",
    "h_effect_size = sms.proportion_effectsize(p_test, p_control)\n",
    "\n",
    "\n",
    "# Define the alpha level (significance level)\n",
    "alpha = 0.05\n",
    "\n",
    "# Calculate the power of the test \n",
    "power_calculated = sms.NormalIndPower().solve_power(effect_size=h_effect_size, nobs1=n_test, alpha=alpha, ratio=n_control/n_test, alternative='two-sided')\n",
    "\n",
    "h_effect_size, power_calculated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228989a2-ecd9-49c9-bab8-160eec70353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd863535-fe41-4223-b4ad-83b14443557a",
   "metadata": {},
   "source": [
    "\n",
    "1. **Effect Size (Cohen's ($h$) )**: Approximately 0.628\n",
    "   - This is a measure of the size of the observed effect in terms of standard deviation units. A value of 0.628 is generally considered a medium to large effect size, indicating a substantial difference between the two groups. This suggests that the difference is not only statistically significant (as indicated by our hypothesis test) but also practically meaningful.\n",
    "   \n",
    "2. **Post-hoc Power**: 1.0 (or 100%)\n",
    "   - This suggests that, with the observed effect size and sample sizes, the test has a 100% chance of correctly rejecting the null hypothesis if it is false, i.e. detecting the observed difference in completion rates between the Test and Control groups.\n",
    "    - However, it's worth noting that a post-hoc power of 1.0 typically indicates a very strong effect, which aligns with the results of our hypothesis test and the calculated effect size. It's also a reminder that post-hoc power analyses mirror the results of the hypothesis test: when you have a significant result, post-hoc power will be high.\n",
    "    - In practice, power analyses are most useful when planning experiments to ensure you have a sufficient sample size to detect an effect of interest. In this case, it confirms that our experiment was well-powered to detect the observed difference. These values give you confidence in the results of the experiment. The sample size is more than sufficient to detect the observed difference between the two groups, and the effect size provides context about how large this difference is in practical terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef47d7fe-ef4e-47c3-9b82-e071adbaf9c4",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57dbd1-9aa8-44db-a131-2e884bf06490",
   "metadata": {},
   "source": [
    "## Designing an experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0179c42a-4bb1-4c5a-bca8-fa9bc9a196d1",
   "metadata": {},
   "source": [
    "Let's assume we are in the phase of designing the experiment for the same hypothesis test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb05dbd-889d-4a24-9896-4e44270dda2d",
   "metadata": {},
   "source": [
    "### Steps for Power Analysis\n",
    "\n",
    "To calculate the minimum sample size for an experiment, especially for a two-proportion z-test, you'd typically need:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf518d-6cc5-4937-a233-ca8e0317203f",
   "metadata": {},
   "source": [
    "1. **Define the Baseline Proportion**:\n",
    "   - This is the expected proportion in the control group, often based on previous data or historical benchmarks. For instance, if historically 74% of users complete the process on your website, that's your baseline proportion.\n",
    "\n",
    "2. **Determine the Anticipated Effect Size**:\n",
    "   - Decide on the smallest difference between the control and test groups that you want to be able to detect. This is the difference from the baseline proportion. If you're testing a new website process design and anticipate a 5% improvement in conversion rate from a baseline of 74%, then the effect size is 5% or 0.05.\n",
    "   - Often referred to as the **Minimum Detectable Effect (MDE)**.\n",
    "\n",
    "3. **Specify Desired Power and Significance Level**:\n",
    "   - **Power**: The probability of correctly rejecting the null hypothesis when the alternative hypothesis is true. Commonly set at 0.80, which means there's an 80% chance of detecting the anticipated effect if it truly exists.\n",
    "   - **Significance Level ($alpha$)**: The risk of a Type I error, which is rejecting the null hypothesis when it's actually true. It's often set at 0.05, denoting a 5% chance of finding an effect that doesn't truly exist.\n",
    "\n",
    "4. **Account for the Ratio of Participants**:\n",
    "   - Specify the ratio of participants between the test and control groups. It's common to have an equal number of participants in both groups (ratio = 1), but sometimes experiments might have imbalanced groups.\n",
    "\n",
    "5. **Determine the Sample Size**:\n",
    "   - Using the above parameters, you can calculate the required sample size for each group using statistical methods or software tools.\n",
    "\n",
    "By following these steps and incorporating the necessary parameters, you can ensure that your experiment is adequately powered to detect the effect size of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392b45e-7000-49d7-a1f6-56f00b8363fb",
   "metadata": {},
   "source": [
    "### Python Code for Power Analysis\n",
    "\n",
    "For this example, let's assume:\n",
    "- You anticipate a 1% difference in conversion rates between the Test and Control groups.\n",
    "    p_control_anticipated = 0.10  # 10%\n",
    "    p_test_anticipated = 0.11  # 11%\n",
    "- You want to have a power of 0.80.\n",
    "- You're using a significance level ($alpha$) of 0.05.\n",
    "- Ratio of Treatment vs. Control: \n",
    "   - This refers to the relative allocation of participants between the two groups. \n",
    "   - A ratio of 50/50 (or 1:1) means the Test group and Control group have an equal number of participants.\n",
    "   - A ratio of 25/75 (or 1:3) means that for every participant in the Test group, there are three in the Control group. The value \\( k \\) often represents this ratio, where $( k = \\frac{\\text{size of Test group}}{\\text{size of Control group}} $).\n",
    "   - This ratio is about how you allocate participants in your experiment.\n",
    "\n",
    "Let's determine the required sample size for each group using these parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9ba99-bc48-46a8-bea6-d029d2001924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the anticipated completion rates for Control and Test groups\n",
    "p_control_anticipated = 0.74  # 74%\n",
    "p_test_anticipated = 0.79  # 79%\n",
    "\n",
    "# Calculate the anticipated effect size\n",
    "anticipated_effect_size = sms.proportion_effectsize(p_control_anticipated, p_test_anticipated)\n",
    "\n",
    "# Specify parameters for power analysis \n",
    "alpha = 0.05 \n",
    "power = 0.8 \n",
    "ratio = 1.0 # 50/50 treatment vs control. if 25-75 treatment vs control then k=0.5 \n",
    "\n",
    "# Using statsmodels to determine the required sample size for a two-proportion z-test\n",
    "required_sample_size = sms.NormalIndPower().solve_power(effect_size=anticipated_effect_size, power=power, alpha=alpha, ratio=ratio, alternative='two-sided')\n",
    "\n",
    "required_sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e209d-26fc-4d9d-87f3-e1f630f39481",
   "metadata": {},
   "source": [
    "The minimum sample size required per group to detect a 5% increase in completion rate (from a baseline of 74% to 79%) with a significance level of ( $alpha$ = 0.05) and a power of 0.80 is approximately 1125 users.\n",
    "\n",
    "This means that to have an 80% chance of detecting a 5% increase in the completion rate, you would need at least 1125 users in both the Test and Control groups.\n",
    "\n",
    "Remember, this calculation assumes that the true proportions in the Test and Control groups will be 79% and 74%, respectively. If the true proportions are different, the actual power of the test will differ from the desired power. Adjusting other parameters, like the desired power or the significance level, will also change the required sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db70267-8079-4906-ae6d-5ff2c89f1d75",
   "metadata": {},
   "source": [
    "### Formula"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92609db-363d-4529-b385-9646a27e363e",
   "metadata": {},
   "source": [
    "The formula for the minimum sample size ( $n $) per group for a two-proportion z-test is:\n",
    "\n",
    "$$[\n",
    "n = \\left( \\frac{{z_{\\alpha/2} + z_{\\beta}}}{{p_{\\text{MDE}}}} \\right)^2 \\times \\left( p_{\\text{baseline}}(1-p_{\\text{baseline}}) + p_{\\text{alt}}(1-p_{\\text{alt}}) \\right)\n",
    "] $$ \n",
    "\n",
    "Where:\n",
    "- $ p_{\\text{MDE}} $ is the Minimum Detectable Effect.\n",
    "- $ p_{\\text{alt}} $ is the proportion in the test group, which is $( p_{\\text{baseline}} + p_{\\text{MDE}} $).\n",
    "- $ z_{\\alpha/2} $ is the z-value associated with a two-tailed test of significance level $( \\alpha $). For $\\alpha = 0.05 $, $ z_{\\alpha/2} $ is approximately 1.96.\n",
    "- $z_{\\beta} $ is the z-value associated with the desired power.\n",
    "\n",
    "Let's calculate the minimum sample size required given a hypothetical baseline proportion and MDE. For demonstration purposes, let's assume:\n",
    "- $ p_{\\text{baseline}} = 0.74 $ (the observed completion rate for the Control group).\n",
    "- We want to detect a 5% increase in completion rate (so $( p_{\\text{MDE}} = 0.05 $)).\n",
    "-  $alpha = 0.05 $ and desired power is 0.80.\n",
    "\n",
    "If instead of doing the above with python functions, you do the raw calculations using the formula, you'll get the same result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
